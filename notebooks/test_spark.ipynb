{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with default configuration:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/07 23:57:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.34 seconds\n",
      "Result: 500000\n",
      "\n",
      "Testing with 2 executors:\n",
      "Time taken: 0.19 seconds\n",
      "Result: 500000\n",
      "\n",
      "Comparison:\n",
      "Default configuration: 0.34 seconds\n",
      "With 2 executors: 0.19 seconds\n",
      "Difference: 0.14 seconds\n",
      "Speedup with executors: 1.75x\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# Function to create SparkSession with specific configuration\n",
    "def create_spark_session(executor_instances=None, app_name=\"SparkTest\"):\n",
    "    builder = SparkSession.builder.appName(app_name)\n",
    "    \n",
    "    if executor_instances:\n",
    "        builder = builder.config(\"spark.executor.instances\", executor_instances)\n",
    "    \n",
    "    return builder.getOrCreate()\n",
    "\n",
    "# Test function using DataFrame operations\n",
    "def run_test():\n",
    "    # Create a large DataFrame and perform operations\n",
    "    df = spark.range(0, 1000000)\n",
    "    \n",
    "    # Perform some CPU-intensive operations\n",
    "    result = df.select(\n",
    "        col(\"id\"),\n",
    "        (col(\"id\") * col(\"id\")).alias(\"squared\"),\n",
    "        (col(\"id\") % 2 == 0).alias(\"is_even\")\n",
    "    ).filter(col(\"is_even\") == True) \\\n",
    "     .count()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with default configuration\n",
    "print(\"Testing with default configuration:\")\n",
    "spark = create_spark_session(app_name=\"DefaultConfig\")\n",
    "start_time = time.time()\n",
    "result1 = run_test()\n",
    "time1 = time.time() - start_time\n",
    "print(f\"Time taken: {time1:.2f} seconds\")\n",
    "print(f\"Result: {result1}\")\n",
    "\n",
    "# Stop the current session\n",
    "spark.stop()\n",
    "\n",
    "# Test with 2 executors\n",
    "print(\"\\nTesting with 2 executors:\")\n",
    "spark = create_spark_session(executor_instances=2, app_name=\"TwoExecutors\")\n",
    "start_time = time.time()\n",
    "result2 = run_test()\n",
    "time2 = time.time() - start_time\n",
    "print(f\"Time taken: {time2:.2f} seconds\")\n",
    "print(f\"Result: {result2}\")\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Default configuration: {time1:.2f} seconds\")\n",
    "print(f\"With 2 executors: {time2:.2f} seconds\")\n",
    "print(f\"Difference: {abs(time2 - time1):.2f} seconds\")\n",
    "if time1 > time2:\n",
    "    print(f\"Speedup with executors: {time1/time2:.2f}x\")\n",
    "else:\n",
    "    print(f\"Slowdown with executors: {time2/time1:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
